## spark-env.sh (generated by Chef)
#  For details refer to (https://github.com/apache/spark/blob/branch-1.6/conf/spark-env.sh.template)

# Set hadoop configuration path and provide CLASSPATH
HADOOP_CONF_DIR=<%= node['pio']['home_prefix'] %>/hadoop/etc/hadoop
SPARK_DIST_CLASSPATH=$(<%= node['pio']['home_prefix'] %>/hadoop/bin/hadoop classpath)

# Setting runtime directories paths both for AIO and standalone,
# since /usr/local/spark is read-only
SPARK_LOG_DIR=<%= node['pio']['libdir'] %>/spark/logs
SPARK_WORKER_DIR=<%= node['pio']['libdir'] %>/spark/work

# Provides path hadoop native libraries to load
SPARK_DAEMON_JAVA_OPTS="-Djava.library.path=<%= node['pio']['home_prefix'] %>/hadoop/lib/native"
